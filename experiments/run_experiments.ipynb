{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9361e58b",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e776ff1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/oktayozel/Desktop/boston university/fall2025/cs599/randomized_svd_implementation_for_large_language_models/CS599-Randomized-SVD\n",
      "Notebook directory: /Users/oktayozel/Desktop/boston university/fall2025/cs599/randomized_svd_implementation_for_large_language_models/CS599-Randomized-SVD/experiments\n",
      "PyTorch version: 2.9.1\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "# Set up paths (works from experiments directory)\n",
    "notebook_dir = Path.cwd()\n",
    "project_root = notebook_dir.parent\n",
    "sys.path.insert(0, str(project_root / 'src'))\n",
    "sys.path.insert(0, str(project_root / 'config'))\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Notebook directory: {notebook_dir}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70f0349",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'measure_training_speed' from 'mla_gpt.utils.metrics' (/Users/oktayozel/Desktop/boston university/fall2025/cs599/randomized_svd_implementation_for_large_language_models/CS599-Randomized-SVD/src/mla_gpt/utils/metrics.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Import experiment modules\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmla_gpt\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexperiment_runner\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ExperimentRunner, ExperimentConfig, ExperimentResults\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmla_gpt\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      4\u001b[39m     compute_perplexity,\n\u001b[32m      5\u001b[39m     measure_training_speed,\n\u001b[32m   (...)\u001b[39m\u001b[32m     10\u001b[39m     compute_reconstruction_error,\n\u001b[32m     11\u001b[39m )\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmla_gpt\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GPT, GPTConfig\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/boston university/fall2025/cs599/randomized_svd_implementation_for_large_language_models/CS599-Randomized-SVD/src/mla_gpt/utils/experiment_runner.py:18\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmla_gpt\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GPT, GPTConfig\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmla_gpt\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     19\u001b[39m     compute_perplexity,\n\u001b[32m     20\u001b[39m     measure_training_speed,\n\u001b[32m     21\u001b[39m     measure_inference_speed,\n\u001b[32m     22\u001b[39m     measure_memory_usage,\n\u001b[32m     23\u001b[39m     measure_compression_metrics,\n\u001b[32m     24\u001b[39m     compare_svd_methods,\n\u001b[32m     25\u001b[39m )\n\u001b[32m     28\u001b[39m \u001b[38;5;129m@dataclass\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mExperimentConfig\u001b[39;00m:\n\u001b[32m     30\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Configuration for a single experiment.\"\"\"\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'measure_training_speed' from 'mla_gpt.utils.metrics' (/Users/oktayozel/Desktop/boston university/fall2025/cs599/randomized_svd_implementation_for_large_language_models/CS599-Randomized-SVD/src/mla_gpt/utils/metrics.py)"
     ]
    }
   ],
   "source": [
    "# Import experiment modules\n",
    "from mla_gpt.utils.experiment_runner import ExperimentRunner, ExperimentConfig, ExperimentResults\n",
    "from mla_gpt.utils.metrics import (\n",
    "    compute_perplexity,\n",
    "    measure_inference_speed,\n",
    "    extract_compression_metrics_from_model,\n",
    "    compute_reconstruction_error,\n",
    ")\n",
    "from mla_gpt.model import GPT, GPTConfig\n",
    "import experiments_config\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"✓ All modules imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb5be86",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "### 1. Select Dataset(s)\n",
    "\n",
    "Choose which dataset(s) to run experiments on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e120c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure datasets to use\n",
    "DATASETS = ['shakespeare_char']  # Options: 'shakespeare_char', 'shakespeare', 'openwebtext'\n",
    "\n",
    "# You can test on multiple datasets:\n",
    "# DATASETS = ['shakespeare_char', 'shakespeare']\n",
    "\n",
    "print(f\"Selected datasets: {DATASETS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b8e44c",
   "metadata": {},
   "source": [
    "### 2. Select Metrics to Collect\n",
    "\n",
    "Choose which metrics you want to collect during experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cea785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure which metrics to collect\n",
    "METRICS_CONFIG = {\n",
    "    # Model quality metrics\n",
    "    'collect_perplexity': True,\n",
    "    'collect_loss': True,\n",
    "    \n",
    "    # Performance metrics\n",
    "    'collect_training_speed': True,\n",
    "    'collect_inference_speed': True,\n",
    "    'collect_time_per_iteration': True,\n",
    "    \n",
    "    # Memory metrics (only on CUDA)\n",
    "    'collect_memory_usage': True,\n",
    "    \n",
    "    # Compression metrics (only for SVD models)\n",
    "    'collect_compression_metrics': True,\n",
    "    'collect_reconstruction_error': True,\n",
    "    'collect_compression_time': True,\n",
    "    \n",
    "    # Training history (stores all values over time)\n",
    "    'save_training_history': True,\n",
    "}\n",
    "\n",
    "# Display selected metrics\n",
    "print(\"Selected metrics:\")\n",
    "for metric, enabled in METRICS_CONFIG.items():\n",
    "    if enabled:\n",
    "        print(f\"  ✓ {metric.replace('_', ' ').title()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf6b493",
   "metadata": {},
   "source": [
    "### 3. Select Experiments to Run\n",
    "\n",
    "Choose from predefined suites or create custom configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f09ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available experiment suites\n",
    "experiments_config.list_suites()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c894886e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option A: Use a predefined suite\n",
    "SUITE_NAME = 'quick'  # Change this: 'quick', 'comparison', 'svd_comparison', 'mla', 'rank_ablation', 'comprehensive'\n",
    "experiment_configs = experiments_config.get_suite(SUITE_NAME)\n",
    "\n",
    "# Option B: Select specific experiments\n",
    "# experiment_configs = [\n",
    "#     experiments_config.BASELINE_SMALL,\n",
    "#     experiments_config.MLA_SMALL,\n",
    "#     experiments_config.RSVD_V_RANK16,\n",
    "# ]\n",
    "\n",
    "# Option C: Create custom configuration\n",
    "# experiment_configs = [\n",
    "#     ExperimentConfig(\n",
    "#         name='my_experiment',\n",
    "#         description='My custom experiment',\n",
    "#         n_layer=4,\n",
    "#         n_head=4,\n",
    "#         n_embd=128,\n",
    "#         block_size=256,\n",
    "#         batch_size=4,\n",
    "#         max_iters=500,\n",
    "#         eval_iters=20,\n",
    "#         use_svd_v=True,\n",
    "#         svd_rank_v=16,\n",
    "#         svd_type='randomized',\n",
    "#     )\n",
    "# ]\n",
    "\n",
    "print(f\"\\nSelected suite: {SUITE_NAME}\")\n",
    "print(f\"Number of experiments: {len(experiment_configs)}\")\n",
    "print(\"\\nExperiments to run:\")\n",
    "for i, config in enumerate(experiment_configs, 1):\n",
    "    print(f\"  {i}. {config.name}: {config.description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f44618",
   "metadata": {},
   "source": [
    "### 4. Configure Output Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ebcec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output configuration\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "OUTPUT_DIR = notebook_dir / 'results'\n",
    "PLOTS_DIR = OUTPUT_DIR / 'plots'\n",
    "\n",
    "# Create directories\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PLOTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Plots directory: {PLOTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d73ccb",
   "metadata": {},
   "source": [
    "## Run Experiments\n",
    "\n",
    "This will run all selected experiments on all selected datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff9373f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize experiment runner\n",
    "runner = ExperimentRunner(\n",
    "    data_dir=str(project_root / 'data'),\n",
    "    output_dir=str(OUTPUT_DIR),\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "print(f\"Experiment runner initialized\")\n",
    "print(f\"Data directory: {project_root / 'data'}\")\n",
    "print(f\"Results will be saved to: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19042d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run experiments on all selected datasets\n",
    "all_results = {}\n",
    "\n",
    "for dataset in DATASETS:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Running experiments on dataset: {dataset}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    results = runner.run_experiments(experiment_configs, dataset=dataset)\n",
    "    all_results[dataset] = results\n",
    "    \n",
    "    print(f\"\\n✓ Completed {len(results)} experiments on {dataset}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"ALL EXPERIMENTS COMPLETED\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Total datasets: {len(DATASETS)}\")\n",
    "print(f\"Total experiments per dataset: {len(experiment_configs)}\")\n",
    "print(f\"Total experiments run: {len(DATASETS) * len(experiment_configs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f863ecc7",
   "metadata": {},
   "source": [
    "## Results Analysis\n",
    "\n",
    "### Create Summary Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03400e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to create DataFrame from results\n",
    "def results_to_dataframe(results_list, dataset_name=''):\n",
    "    \"\"\"Convert list of ExperimentResults to pandas DataFrame.\"\"\"\n",
    "    data = []\n",
    "    for result in results_list:\n",
    "        row = {\n",
    "            'dataset': dataset_name,\n",
    "            'name': result.config.name,\n",
    "            'description': result.config.description,\n",
    "            'model_params': result.model_params,\n",
    "            'train_loss': result.final_train_loss,\n",
    "            'val_loss': result.final_val_loss,\n",
    "            'train_ppl': result.final_train_perplexity,\n",
    "            'val_ppl': result.final_val_perplexity,\n",
    "            'train_tokens_per_sec': result.training_tokens_per_sec,\n",
    "            'inference_tokens_per_sec': result.inference_tokens_per_sec,\n",
    "            'forward_memory_mb': result.forward_memory_mb,\n",
    "            'backward_memory_mb': result.backward_memory_mb,\n",
    "            'total_memory_mb': result.total_memory_mb,\n",
    "            'total_training_time': result.total_training_time,\n",
    "        }\n",
    "        data.append(row)\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Create DataFrames for each dataset\n",
    "dfs = {}\n",
    "for dataset, results in all_results.items():\n",
    "    dfs[dataset] = results_to_dataframe(results, dataset)\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Results Summary for {dataset}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    display(dfs[dataset])\n",
    "\n",
    "# Combine all datasets\n",
    "combined_df = pd.concat(dfs.values(), ignore_index=True)\n",
    "\n",
    "# Save to CSV\n",
    "csv_path = OUTPUT_DIR / f'{SUITE_NAME}_all_datasets_summary.csv'\n",
    "combined_df.to_csv(csv_path, index=False)\n",
    "print(f\"\\n✓ Combined summary saved to: {csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820c0155",
   "metadata": {},
   "source": [
    "### Metrics Comparison Across Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06390fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare same model across different datasets\n",
    "if len(DATASETS) > 1:\n",
    "    print(\"\\nModel Performance Across Datasets:\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    pivot_ppl = combined_df.pivot(index='name', columns='dataset', values='val_ppl')\n",
    "    print(\"\\nValidation Perplexity:\")\n",
    "    display(pivot_ppl)\n",
    "    \n",
    "    pivot_speed = combined_df.pivot(index='name', columns='dataset', values='inference_tokens_per_sec')\n",
    "    print(\"\\nInference Speed (tokens/sec):\")\n",
    "    display(pivot_speed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6153d717",
   "metadata": {},
   "source": [
    "## Visualizations\n",
    "\n",
    "### 1. Training and Validation Loss Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6558f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "if METRICS_CONFIG['save_training_history']:\n",
    "    for dataset, results in all_results.items():\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        fig.suptitle(f'Loss Curves - {dataset}', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        # Training loss\n",
    "        ax = axes[0]\n",
    "        for result in results:\n",
    "            if result.train_losses:\n",
    "                ax.plot(result.train_losses, label=result.config.name, alpha=0.7, linewidth=2)\n",
    "        ax.set_xlabel('Iteration', fontsize=12)\n",
    "        ax.set_ylabel('Training Loss', fontsize=12)\n",
    "        ax.set_title('Training Loss')\n",
    "        ax.legend(loc='best')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Validation loss\n",
    "        ax = axes[1]\n",
    "        for result in results:\n",
    "            if result.val_losses:\n",
    "                eval_interval = result.config.eval_interval\n",
    "                x = np.arange(len(result.val_losses)) * eval_interval\n",
    "                ax.plot(x, result.val_losses, label=result.config.name, \n",
    "                       alpha=0.7, marker='o', linewidth=2, markersize=4)\n",
    "        ax.set_xlabel('Iteration', fontsize=12)\n",
    "        ax.set_ylabel('Validation Loss', fontsize=12)\n",
    "        ax.set_title('Validation Loss')\n",
    "        ax.legend(loc='best')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(PLOTS_DIR / f'{dataset}_loss_curves.png', dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(f\"✓ Saved: {PLOTS_DIR / f'{dataset}_loss_curves.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7723134",
   "metadata": {},
   "source": [
    "### 2. Perplexity Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31b1670",
   "metadata": {},
   "outputs": [],
   "source": [
    "if METRICS_CONFIG['collect_perplexity']:\n",
    "    for dataset, df in dfs.items():\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        \n",
    "        x = np.arange(len(df))\n",
    "        width = 0.35\n",
    "        \n",
    "        bars1 = ax.bar(x - width/2, df['train_ppl'], width, \n",
    "                      label='Train Perplexity', alpha=0.8, color='steelblue')\n",
    "        bars2 = ax.bar(x + width/2, df['val_ppl'], width, \n",
    "                      label='Val Perplexity', alpha=0.8, color='coral')\n",
    "        \n",
    "        ax.set_xlabel('Model', fontsize=12)\n",
    "        ax.set_ylabel('Perplexity (lower is better)', fontsize=12)\n",
    "        ax.set_title(f'Perplexity Comparison - {dataset}', fontsize=14, fontweight='bold')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(df['name'], rotation=45, ha='right')\n",
    "        ax.legend(fontsize=11)\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bars in [bars1, bars2]:\n",
    "            for bar in bars:\n",
    "                height = bar.get_height()\n",
    "                ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                       f'{height:.1f}', ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(PLOTS_DIR / f'{dataset}_perplexity.png', dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(f\"✓ Saved: {PLOTS_DIR / f'{dataset}_perplexity.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1ac38f",
   "metadata": {},
   "source": [
    "### 3. Speed Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdff760",
   "metadata": {},
   "outputs": [],
   "source": [
    "if METRICS_CONFIG['collect_training_speed'] and METRICS_CONFIG['collect_inference_speed']:\n",
    "    for dataset, df in dfs.items():\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        \n",
    "        x = np.arange(len(df))\n",
    "        width = 0.35\n",
    "        \n",
    "        bars1 = ax.bar(x - width/2, df['train_tokens_per_sec'], width, \n",
    "                      label='Training Speed', alpha=0.8, color='steelblue')\n",
    "        bars2 = ax.bar(x + width/2, df['inference_tokens_per_sec'], width, \n",
    "                      label='Inference Speed', alpha=0.8, color='coral')\n",
    "        \n",
    "        ax.set_xlabel('Model', fontsize=12)\n",
    "        ax.set_ylabel('Tokens per Second (higher is better)', fontsize=12)\n",
    "        ax.set_title(f'Training and Inference Speed - {dataset}', fontsize=14, fontweight='bold')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(df['name'], rotation=45, ha='right')\n",
    "        ax.legend(fontsize=11)\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Add value labels\n",
    "        for bars in [bars1, bars2]:\n",
    "            for bar in bars:\n",
    "                height = bar.get_height()\n",
    "                ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                       f'{height:.0f}', ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(PLOTS_DIR / f'{dataset}_speed.png', dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(f\"✓ Saved: {PLOTS_DIR / f'{dataset}_speed.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4342c5",
   "metadata": {},
   "source": [
    "### 4. Memory Usage Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436a356b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if METRICS_CONFIG['collect_memory_usage'] and DEVICE == 'cuda':\n",
    "    for dataset, df in dfs.items():\n",
    "        if df['total_memory_mb'].sum() > 0:  # Only plot if we have memory data\n",
    "            fig, ax = plt.subplots(figsize=(12, 6))\n",
    "            \n",
    "            x = np.arange(len(df))\n",
    "            width = 0.25\n",
    "            \n",
    "            bars1 = ax.bar(x - width, df['forward_memory_mb'], width, \n",
    "                          label='Forward Pass', alpha=0.8)\n",
    "            bars2 = ax.bar(x, df['backward_memory_mb'], width, \n",
    "                          label='Backward Pass', alpha=0.8)\n",
    "            bars3 = ax.bar(x + width, df['total_memory_mb'], width, \n",
    "                          label='Total', alpha=0.8)\n",
    "            \n",
    "            ax.set_xlabel('Model', fontsize=12)\n",
    "            ax.set_ylabel('Memory Usage (MB, lower is better)', fontsize=12)\n",
    "            ax.set_title(f'Memory Usage - {dataset}', fontsize=14, fontweight='bold')\n",
    "            ax.set_xticks(x)\n",
    "            ax.set_xticklabels(df['name'], rotation=45, ha='right')\n",
    "            ax.legend(fontsize=11)\n",
    "            ax.grid(True, alpha=0.3, axis='y')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(PLOTS_DIR / f'{dataset}_memory.png', dpi=150, bbox_inches='tight')\n",
    "            plt.show()\n",
    "            print(f\"✓ Saved: {PLOTS_DIR / f'{dataset}_memory.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092a9d84",
   "metadata": {},
   "source": [
    "### 5. Quality vs Speed Tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f8ecd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset, df in dfs.items():\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    fig.suptitle(f'Quality vs Speed Tradeoff - {dataset}', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Training speed vs perplexity\n",
    "    ax = axes[0]\n",
    "    scatter1 = ax.scatter(df['train_tokens_per_sec'], df['val_ppl'], \n",
    "                         s=df['model_params']/100, alpha=0.6, \n",
    "                         c=range(len(df)), cmap='viridis')\n",
    "    for i, name in enumerate(df['name']):\n",
    "        ax.annotate(name, (df.iloc[i]['train_tokens_per_sec'], df.iloc[i]['val_ppl']),\n",
    "                   fontsize=9, alpha=0.8, xytext=(5, 5), textcoords='offset points')\n",
    "    ax.set_xlabel('Training Speed (tokens/sec, higher is better)', fontsize=11)\n",
    "    ax.set_ylabel('Validation Perplexity (lower is better)', fontsize=11)\n",
    "    ax.set_title('Quality vs Training Speed')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Inference speed vs perplexity\n",
    "    ax = axes[1]\n",
    "    scatter2 = ax.scatter(df['inference_tokens_per_sec'], df['val_ppl'], \n",
    "                         s=df['model_params']/100, alpha=0.6, \n",
    "                         c=range(len(df)), cmap='viridis')\n",
    "    for i, name in enumerate(df['name']):\n",
    "        ax.annotate(name, (df.iloc[i]['inference_tokens_per_sec'], df.iloc[i]['val_ppl']),\n",
    "                   fontsize=9, alpha=0.8, xytext=(5, 5), textcoords='offset points')\n",
    "    ax.set_xlabel('Inference Speed (tokens/sec, higher is better)', fontsize=11)\n",
    "    ax.set_ylabel('Validation Perplexity (lower is better)', fontsize=11)\n",
    "    ax.set_title('Quality vs Inference Speed')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(PLOTS_DIR / f'{dataset}_tradeoff.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"✓ Saved: {PLOTS_DIR / f'{dataset}_tradeoff.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1b2913",
   "metadata": {},
   "source": [
    "### 6. Compression Metrics (for SVD models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c6ef47",
   "metadata": {},
   "outputs": [],
   "source": [
    "if METRICS_CONFIG['collect_compression_metrics']:\n",
    "    for dataset, results in all_results.items():\n",
    "        # Extract compression metrics\n",
    "        compression_data = []\n",
    "        for result in results:\n",
    "            if result.compression_metrics:\n",
    "                for layer, metrics in result.compression_metrics.items():\n",
    "                    compression_data.append({\n",
    "                        'model': result.config.name,\n",
    "                        'layer': layer,\n",
    "                        'relative_error': metrics.get('relative_error', 0),\n",
    "                        'compression_time': metrics.get('compression_time', 0),\n",
    "                        'compression_ratio': metrics.get('compression_ratio', 1),\n",
    "                    })\n",
    "        \n",
    "        if compression_data:\n",
    "            comp_df = pd.DataFrame(compression_data)\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"Compression Metrics - {dataset}\")\n",
    "            print(f\"{'='*80}\")\n",
    "            display(comp_df)\n",
    "            \n",
    "            # Plot compression metrics\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "            fig.suptitle(f'SVD Compression Metrics - {dataset}', fontsize=14, fontweight='bold')\n",
    "            \n",
    "            # Compression error\n",
    "            ax = axes[0]\n",
    "            comp_df.groupby('model')['relative_error'].mean().plot(\n",
    "                kind='bar', ax=ax, alpha=0.8, color='steelblue'\n",
    "            )\n",
    "            ax.set_xlabel('Model', fontsize=11)\n",
    "            ax.set_ylabel('Relative Reconstruction Error (lower is better)', fontsize=11)\n",
    "            ax.set_title('SVD Reconstruction Error')\n",
    "            ax.tick_params(axis='x', rotation=45)\n",
    "            ax.grid(True, alpha=0.3, axis='y')\n",
    "            \n",
    "            # Compression time\n",
    "            ax = axes[1]\n",
    "            comp_df.groupby('model')['compression_time'].mean().plot(\n",
    "                kind='bar', ax=ax, alpha=0.8, color='coral'\n",
    "            )\n",
    "            ax.set_xlabel('Model', fontsize=11)\n",
    "            ax.set_ylabel('Compression Time (sec, lower is better)', fontsize=11)\n",
    "            ax.set_title('SVD Compression Time')\n",
    "            ax.tick_params(axis='x', rotation=45)\n",
    "            ax.grid(True, alpha=0.3, axis='y')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(PLOTS_DIR / f'{dataset}_compression.png', dpi=150, bbox_inches='tight')\n",
    "            plt.show()\n",
    "            print(f\"✓ Saved: {PLOTS_DIR / f'{dataset}_compression.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933fed1e",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92da6e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print comprehensive summary\n",
    "print(\"=\"*80)\n",
    "print(\"EXPERIMENT SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nSuite: {SUITE_NAME}\")\n",
    "print(f\"Datasets: {', '.join(DATASETS)}\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Total experiments: {len(DATASETS) * len(experiment_configs)}\")\n",
    "\n",
    "for dataset, df in dfs.items():\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"BEST MODELS - {dataset}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    best_ppl_idx = df['val_ppl'].idxmin()\n",
    "    print(f\"\\nBest Perplexity: {df.iloc[best_ppl_idx]['name']}\")\n",
    "    print(f\"  Val Perplexity: {df.iloc[best_ppl_idx]['val_ppl']:.2f}\")\n",
    "    \n",
    "    best_train_speed_idx = df['train_tokens_per_sec'].idxmax()\n",
    "    print(f\"\\nFastest Training: {df.iloc[best_train_speed_idx]['name']}\")\n",
    "    print(f\"  Training Speed: {df.iloc[best_train_speed_idx]['train_tokens_per_sec']:.0f} tokens/sec\")\n",
    "    \n",
    "    best_inference_speed_idx = df['inference_tokens_per_sec'].idxmax()\n",
    "    print(f\"\\nFastest Inference: {df.iloc[best_inference_speed_idx]['name']}\")\n",
    "    print(f\"  Inference Speed: {df.iloc[best_inference_speed_idx]['inference_tokens_per_sec']:.0f} tokens/sec\")\n",
    "    \n",
    "    if df['total_memory_mb'].max() > 0:\n",
    "        best_memory_idx = df['total_memory_mb'].idxmin()\n",
    "        print(f\"\\nLowest Memory Usage: {df.iloc[best_memory_idx]['name']}\")\n",
    "        print(f\"  Total Memory: {df.iloc[best_memory_idx]['total_memory_mb']:.2f} MB\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75358c81",
   "metadata": {},
   "source": [
    "## Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c2799c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive export\n",
    "export_data = {\n",
    "    'suite_name': SUITE_NAME,\n",
    "    'datasets': DATASETS,\n",
    "    'device': DEVICE,\n",
    "    'metrics_config': METRICS_CONFIG,\n",
    "    'num_experiments_per_dataset': len(experiment_configs),\n",
    "    'total_experiments': len(DATASETS) * len(experiment_configs),\n",
    "    'results_by_dataset': {}\n",
    "}\n",
    "\n",
    "for dataset, df in dfs.items():\n",
    "    export_data['results_by_dataset'][dataset] = df.to_dict('records')\n",
    "\n",
    "export_path = OUTPUT_DIR / f'{SUITE_NAME}_complete_export.json'\n",
    "with open(export_path, 'w') as f:\n",
    "    json.dump(export_data, f, indent=2)\n",
    "\n",
    "print(f\"✓ Exported complete results to: {export_path}\")\n",
    "print(f\"✓ All plots saved to: {PLOTS_DIR}\")\n",
    "print(f\"✓ CSV summary saved to: {OUTPUT_DIR / f'{SUITE_NAME}_all_datasets_summary.csv'}\")\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"EXPERIMENTS COMPLETE! ✓\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6755d884",
   "metadata": {},
   "source": [
    "## Custom Analysis\n",
    "\n",
    "Use this section for custom analysis and visualizations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cc9260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your custom analysis here\n",
    "# For example:\n",
    "# - Compare specific models\n",
    "# - Create custom visualizations\n",
    "# - Statistical tests\n",
    "# - Export data in different formats"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "599env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
